# -*- coding: utf-8 -*-
"""TP2RN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yr86cl8_4us41oZl1uSAlhAZ2XZUCRqQ
"""

!pip install pandas
!pip install tensorflow
!pip install sklearn
!pip install matplotlib
!pip install pretty-confusion-matrix

import pandas as pd
import os
import tensorflow as tf
import numpy as np

#Q1 :
iris_data = pd.read_csv("Iris.csv")

#Question 2 :
iris_data.head(10)

#Question 3 :
#Remarque : Le premier est le nombre de lignes et l'autre est le nombre de colonnes.
#Remarque : on peut utiliser aussi dim(iris_data)
iris_data.shape

#Question 4 :
#seaborn est un outil Python pour créer de jolis tracés et figures. 
#Il inclut certaines fonctionnalités liées au dataset Iris.
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn
seaborn.set_style('whitegrid')
df = seaborn.load_dataset("iris")
seaborn.pairplot(df, hue="species");

#Question5
iris_data.loc[iris_data["Species"] == "Iris-setosa" , "Species"] = 0
iris_data.loc[iris_data["Species"] == "Iris-versicolor" , "Species"] = 1
iris_data.loc[iris_data["Species"] == "Iris-virginica" , "Species"] = 2

#Question 6 :
iris_data.head(10)

#Question 7 :
from sklearn.model_selection import train_test_split
X_data = iris_data.iloc[:, 1:5].values
Y_data = iris_data.iloc[:, 5].values
X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, test_size=0.3)# le reste des données pour l'apprentissage

#Question 8
print("\n 10 premières données d’apprentissage et celles de test (X):\n------------------------------------")
print(X_train[0:10], '\n\n', X_test[0:10])
print("\n 10 premières données d’apprentissage et celles de test (Y):\n------------------------------------")
print(Y_train[0:10], '\n\n', Y_test[0:10])

#Question 9
from sklearn.neural_network import MLPClassifier
classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 3), epsilon=0.07, max_iter=150)
classifier.fit(X_train, Y_train.astype('int'))

#Question 10
prediction = classifier.predict(X_test)
print(prediction)

Y_test.astype('int')

from sklearn import metrics
print('The accuracy of the Multi-layer Perceptron is:',metrics.accuracy_score(prediction,Y_test.astype('int')))

#Question 11
import matplotlib.pyplot as plt
import numpy
confusion_mat =metrics.confusion_matrix(Y_test.tolist(), prediction.tolist())

fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(confusion_mat)
fig.colorbar(cax)

plt.show()

# deuxiéme bib pour la matrice de confusion
from pretty_confusion_matrix import pp_matrix_from_data
t = np.array(Y_test.tolist())
p = np.array(prediction.tolist())
pp_matrix_from_data(t, p)

#Question 12
#Modèle d'apprentissage caractérisé par une accurancy élevée

"""#Question 12
Modèle d'apprentissage caractérisé par une accurancy élevée = 93,33 % 


1.   Pour les individus de la classe 0 ( Iris_Setosa ) , 17 sur 17 ont bien été identifiés comme appartenant à cette classe
2.   Pour les individus de la classe 1 (Iris_Versicolor), 12 sur 13 ont bien été identifiés.
3.  Pour les individus de la classe 2 (Iris_Virginica), 13 sur 15 ont bien été identifiés.

**calcule précision(class)** 

precision(0) = 17/17 = 100%

precision(1) = 12/13 = 92.3%

precision(2) = 13/15 = 86.66%

**calcule Recall(class)** 

Recall(0) = 17/17 = 100%

Recall(1) = 12/13 = 92.3%

Recall(2) = 13/15 = 86.66%

"""

#Question 13
# different learning rate schedules and momentum parameters
params = [
    {
        "solver": "sgd",
        "learning_rate": "constant",
        "learning_rate_init": 0.2,
        "max_iter" : 150
    },
    {
        "solver": "sgd",
        "learning_rate": "constant",
        "learning_rate_init": 0.7,
        "max_iter" : 150
    },
    {
        "solver": "sgd",
        "learning_rate": "invscaling",
        "learning_rate_init": 0.2,
        "max_iter" : 150
    },
    {
        "solver": "sgd",
        "learning_rate": "invscaling",
        "learning_rate_init": 0.7,
        "max_iter" : 150
    },
    {"solver": "adam", "learning_rate_init": 0.01},
]

labels = [
    "constant learning-rate 0.2",
    "constant learning-rate 0.7",
    "inv-scaling learning-rate 0.2",
    "inv-scaling learning-rate 0.7",
    "adam 0.01",
]

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

scaler = sc.fit(X_train)
trainX_scaled = scaler.transform(X_train)
testX_scaled = scaler.transform(X_test)

dataClassifiers = []
for i in range(len(params)):
  classifier = MLPClassifier(random_state=0, **params[i])
  classifier.fit(X_train, Y_train.astype('int'))
  print(labels[i]," : ",classifier.score(X_train, Y_train.astype('int')))
  dataClassifiers.append(classifier)

#Question 14

plot_args = [
    {"c": "red", "linestyle": "-"},
    {"c": "green", "linestyle": "-"},
    {"c": "blue", "linestyle": "-"},
    {"c": "red", "linestyle": "--"},
    {"c": "green", "linestyle": "--"},
]

for i in range(len(dataClassifiers)):
  plt.plot(dataClassifiers[i].loss_curve_, **plot_args[i])
  plt.title(labels[i], fontsize=14)
  plt.show()

#Question 15
from sklearn import preprocessing
from tensorflow import keras

label_encoder = preprocessing.LabelEncoder()
iris_data['Species'] = label_encoder.fit_transform(
                                iris_data['Species'])
np_iris = iris_data.to_numpy()
X_data = np_iris[:,0:4]
Y_data=np_iris[:,4]
scaler = StandardScaler().fit(X_data)
X_data = scaler.transform(X_data)
Y_data = tf.keras.utils.to_categorical(Y_data,3)
X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, test_size=0.3)


#Number of classes in the target variable
NB_CLASSES=3

#Create a sequencial model in Keras
model = tf.keras.models.Sequential()

#Add the first hidden layer
model.add(keras.layers.Dense(128,                    #Number of nodes
                             input_shape=(4,),       #Number of input variables
                              name='Hidden-Layer-1', #Logical name
                              activation='relu'))    #activation function

#Add a second hidden layer
model.add(keras.layers.Dense(128,
                              name='Hidden-Layer-2',
                              activation='relu'))

#Add an output layer with softmax activation
model.add(keras.layers.Dense(NB_CLASSES,
                             name='Output-Layer',
                             activation='softmax'))

#Compile the model with loss & metrics
model.compile(loss='categorical_crossentropy',
              metrics=['accuracy'])

#Print the model meta-data
model.summary()

"""Le phénomène constaté est le surapprentissage (overfitting).
La solution pour ce problème est de diviser la base en deux parties: base d'apprentissage est base de test.
"""

#Make it verbose so we can see the progress
VERBOSE=1

#Setup Hyper Parameters for training

#Set Batch size
BATCH_SIZE=16
#Set number of epochs
EPOCHS=10
#Set validation split. 20% of the training data will be used for validation
#after each epoch
VALIDATION_SPLIT=0.2

print("\nTraining Progress:\n------------------------------------")

#Fit the model. This will perform the entire training cycle, including
#forward propagation, loss computation, backward propagation and gradient descent.
#Execute for the specified batch sizes and epoch
#Perform validation after each epoch 
history=model.fit(X_train,
          Y_train.astype('int'),
          batch_size=BATCH_SIZE,
          epochs=EPOCHS,
          verbose=VERBOSE,
          validation_split=VALIDATION_SPLIT)

print("\nAccuracy during Training :\n------------------------------------")
import matplotlib.pyplot as plt

#Plot accuracy of the model after each epoch.
pd.DataFrame(history.history)["accuracy"].plot(figsize=(8, 5))
plt.title("Accuracy improvements with Epoch")
plt.show()

#Evaluate the model against the test dataset and print results
print("\nEvaluation against Test Dataset :\n------------------------------------")
model.evaluate(X_test,Y_test)

#Make it verbose so we can see the progress
VERBOSE=1

#Setup Hyper Parameters for training

#Set Batch size
BATCH_SIZE=16
#Set number of epochs
EPOCHS=100
#Set validation split. 20% of the training data will be used for validation
#after each epoch
VALIDATION_SPLIT=0.2

print("\nTraining Progress:\n------------------------------------")

#Fit the model. This will perform the entire training cycle, including
#forward propagation, loss computation, backward propagation and gradient descent.
#Execute for the specified batch sizes and epoch
#Perform validation after each epoch 
history=model.fit(X_train,
          Y_train.astype('int'),
          batch_size=BATCH_SIZE,
          epochs=EPOCHS,
          verbose=VERBOSE,
          validation_split=VALIDATION_SPLIT)

print("\nAccuracy during Training :\n------------------------------------")
import matplotlib.pyplot as plt

#Plot accuracy of the model after each epoch.
pd.DataFrame(history.history)["accuracy"].plot(figsize=(8, 5))
plt.title("Accuracy improvements with Epoch")
plt.show()

#Evaluate the model against the test dataset and print results
print("\nEvaluation against Test Dataset :\n------------------------------------")
model.evaluate(X_test,Y_test)

#enregistrer le model
model.save("iris_save")
loaded_model = keras.models.load_model("iris_save")
loaded_model.summary()

#Raw prediction data
prediction_input = [[5.1,3.5,1.4,0.2]]
scaled_input = scaler.transform(prediction_input)
#Get raw prediction probabilities
raw_prediction = model.predict(scaled_input)
print("Raw Prediction Output (Probabilities) :" , raw_prediction)
#Find prediction
prediction = np.argmax(raw_prediction)
print("Prediction is ", label_encoder.inverse_transform([prediction]))

#Raw prediction data
prediction_input = [[5.1,3.5,1.4,17]]
scaled_input = scaler.transform(prediction_input)
#Get raw prediction probabilities
raw_prediction = model.predict(scaled_input)
print("Raw Prediction Output (Probabilities) :" , raw_prediction)
#Find prediction
prediction = np.argmax(raw_prediction)
print("Prediction is ", label_encoder.inverse_transform([prediction]))



